{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a52cff",
   "metadata": {},
   "source": [
    "# Part 3: Hands-On - End-to-end Workflow on Titanic Dataset\n",
    "\n",
    "**üéØ Goal**: We will apply a Supervised Learning workflow on the famous Titanic dataset to predict the survival of passengers.\n",
    "\n",
    "**üóíÔ∏è Scenario**\n",
    "\n",
    "The Titanic dataset contains information about the passengers, which includes their age, sex, ticket class, and whether they survived the sinking of the Titanic. Refer to the description and data dictionary: https://www.kaggle.com/competitions/titanic/data\n",
    "\n",
    "**‚ö° Task**\n",
    "\n",
    "## 1. Imports and Data Loading\n",
    "\n",
    "First, we import the necessary libraries. seaborn is used here specifically to easily load the built-in Titanic dataset. We then inspect the dataframe to understand its structure and check for missing values, which determines our preprocessing strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ae7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b8a715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Titanic dataset from the seaborn library\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0f08b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows to inspect data types and example values\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f9ac55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0    549\n",
       "1    342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of the target variable to see if classes are balanced\n",
    "titanic.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863a2c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify columns with missing values to decide on imputation strategies\n",
    "# Note: 'age' and 'deck' have significant missing data\n",
    "\n",
    "titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34ef5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae6f4a",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Splitting\n",
    "\n",
    "We select a specific subset of features to train our model. We separate the data into the feature matrix (X) and the target vector (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "178734e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features: mixture of numerical (age, fare) and categorical (sex, embaked)\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "target = 'survived'\n",
    "\n",
    "# Split data into Features (X) and Target (y)\n",
    "X = titanic[features]\n",
    "y = titanic[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9170f3",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Building the Pre-Processing Pipeline\n",
    "\n",
    "This is the core of the workflow. We use Pipeline to chain sequential steps (like imputation and scaling) and ColumnTransformer to apply these different pipelines to specific columns (numerical vs. categorical) simultaneously.\n",
    "\n",
    "- **Numerical Data**: We fill missing values with the median and scale data to unit variance.\n",
    "\n",
    "- **Categorical Data**: We fill missing values with the most frequent value and convert text categories into binary vectors (One-Hot Encoding).\n",
    "\n",
    "`sklearn`'s pipeline is a tool that allows us to assemble several steps together. It sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‚Äòtransforms‚Äô, that is, they must implement fit and transform methods. The final estimator only needs to implement fit.\n",
    "\n",
    "![Pipeline Overview](../assets/titanic-data-pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4706ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # Fill missing values with median\n",
    "    ('scaler', StandardScaler())                   # Standardize features (mean=0, variance=1)\n",
    "])\n",
    "\n",
    "# 2. Define pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')), # Fill missing with mode\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))    # Convert categories to binary vectors\n",
    "])\n",
    "\n",
    "# 3. Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply numerical pipeline to specific numeric columns\n",
    "        ('num', numerical_transformer, ['age', 'sibsp', 'parch', 'fare']),\n",
    "        # Apply categorical pipeline to specific categorical columns\n",
    "        ('cat', categorical_transformer, ['pclass', 'sex', 'embarked'])\n",
    "    ])\n",
    "\n",
    "# 4. Create the full end-to-end pipeline including the model\n",
    "# This ensures raw data flows through preprocessing directly into the model\n",
    "model = LogisticRegression()\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model', model)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f4ea55",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation\n",
    "\n",
    "Finally, we split the data into training and testing sets. We fit the entire pipeline on the training data and evaluate its performance on the unseen test data using various classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0aca516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 0.75\n",
      "Recall: 0.74\n",
      "F1 Score: 0.74\n",
      "AUC: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Split data: 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "\n",
    "# Calculate ROC curve and AUC score\n",
    "# Note: We use predict_proba for ROC/AUC to get probability scores instead of class labels\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pipeline.predict_proba(X_test)[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Output results\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f\"AUC: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684d4cd",
   "metadata": {},
   "source": [
    "## Exercise (Self-Study)\n",
    "\n",
    "> Now, recreate the workflow but use min-max scaling for numerical features and KNN classifier for model.\n",
    "\n",
    "We redefine the numerical pipeline to use `MinMaxScaler` instead of `StandardScaler`. We then integrate this into a new preprocessor and combine it with a KNeighborsClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5834fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 1. Redefine numerical pipeline with Min-Max Scaling\n",
    "# Min-Max scaling scales data to a fixed range [0, 1], which preserves the shape of the original distribution\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # Fill missing values\n",
    "    ('scaler', MinMaxScaler())                     # Scale to range [0, 1]\n",
    "])\n",
    "\n",
    "# 2. Update the ColumnTransformer\n",
    "# We reuse the 'categorical_transformer' defined in the previous section (Imputer + OneHotEncoder)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, ['age', 'sibsp', 'parch', 'fare']),\n",
    "        ('cat', categorical_transformer, ['pclass', 'sex', 'embarked'])\n",
    "    ])\n",
    "\n",
    "# 3. Define the new model: K-Nearest Neighbors\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# 4. Create the new pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model', model)\n",
    "                          ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9de4de",
   "metadata": {},
   "source": [
    "**Training and Evaluation**\n",
    "\n",
    "We fit this new pipeline to the same training data used previously and evaluate its performance. This allows for a direct comparison between the Logistic Regression (StandardScaler) approach and this KNN (MinMaxScaler) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec64d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the KNN pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "preds = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "precision = precision_score(y_test, preds)\n",
    "recall = recall_score(y_test, preds)\n",
    "f1 = f1_score(y_test, preds)\n",
    "\n",
    "# Calculate ROC/AUC\n",
    "# Note: KNN supports predict_proba, which allows us to calculate AUC just like Logistic Regression\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pipeline.predict_proba(X_test)[:,1])\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28081332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n",
      "Precision: 0.79\n",
      "Recall: 0.70\n",
      "F1 Score: 0.74\n",
      "AUC: 0.85\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f\"AUC: {roc_auc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
